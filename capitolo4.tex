\chapter{L'implementazione in OpenCL}
\vspace{4cm}


\section{Generatore dei task}
Al fine di poter comparare i risultati ottenuti nel paper[jcssp] con i nostri, si è deciso di implementare un generatore di task con caratteristiche simili a quelle del paper, che fornisca in output una varietà di DAG su cui poter eseguire i test.
\\
Il generatore dipende da diversi parametri di input che sono:
\begin{itemize}
	\item{Number of tasks in the graph ($v$)}
	\item{Out degree ($\beta$)}
	\item{Shape parameter of a graph ($\alpha$)}
	\item{Communication to Computation Ratio (CCR)}
	\item{Range percentage of computation cost ($\eta$)}
\end{itemize}

In particolare, l'altezza del grafo è generata randomicamente a partire da una distribuzione uniforme con valore medio pari a $\frac{\sqrt{v}}{\alpha}$, mentre l'ampiezza a partire da una distribuzione uniforme con valore medio pari a $\sqrt{v}\times\alpha$.
In questo modo si può generare un grafo più o meno denso semplicemente modificando il parametro $\alpha$.

%TODO: Aggiungere descrizione parametro beta

Inoltre il parametro CCR indica quanto i task sono impattanti dal punto di vista computazionale rispetto alla mole di dati che analizzano, se CCR è molto basso il costo computazionale è più alto rispetto alla quantità di dati trasmessi ai task successori pertanto l'applicazione può essere considerata ad alta intensità di calcolo, viceversa se CCR è alto l'applicazione trasmette molti dati tra i task ma non è molto pesante dal punto di vista computazionale.

Infine il parametro $\eta$ indica il grado di eterogeneità del sistema, cioè se ci sono differenze significative tra le prestazioni dei processori, se $\eta$ è alto i costi dei task sui processori variano molto di processore in processore, viceversa se $\eta$ basso tutti i processori completeranno lo stesso task in tempi uguali. 
A partire da questo, il costo medio della computazione $W\ped{i}$ per ogni task è stato scelto casualmente da una distribuzione uniforme con estremi $0$ e $2 \times \text{Wdag}$, dove Wdag è una costante del generatore che indica il costo medio di computazione dei task del grafo, e il costo computazionale $W\ped{i,j}$ di ogni task $v\ped{i}$ su ogni processore $p\ped{j}$ è scelto randomicamente dall'intervallo $[W\ped{i}\times (1-\frac{\eta}{2}), W\ped{i}\times (1+\frac{\eta}{2})]$.

\section{Fasi dell'algoritmo}
Al fine di implementare attraverso OpenCL una versione parallelizzata dell'algoritmo PETS, si è suddiviso l'algoritmo in 4 fasi:

\begin{enumerate}
	\item Ricerca degli entrypoint
	\item Calcolo dei livelli e dei ranghi
	\item Ordinamento dei task
	\item Selezione del processore
\end{enumerate}


Per ogni fase è stato implementato un kernel OpenCL da eseguire in parallelo su GPU.

\subsection{Ricerca degli entrypoint}
\begin{lstlisting}[language=C++, caption={Find entrypoints kernel II},captionpos=b]
	kernel void entry_discover_rectangular(const int n_nodes, global edge_t* restrict edges, volatile global int* n_entries, global int* entries)
	{
		int current_node_index = get_global_id(0);
		if (current_node_index >= n_nodes) return;
		
		if (edges[matrixToArrayIndex] <= -1)
		entries[i] = 1;
	}
\end{lstlisting}

\subsection{Calcolo dei livelli e dei ranghi}
\begin{lstlisting}[language=C++, caption={Compute metrics kernel II},captionpos=b]
	kernel void compute_metrics_rectangular(global int* restrict nodes, global int* queue_, global int* next_queue_, const int n_nodes, global edge_t* restrict edges, global edge_t* restrict edges_reverse, volatile global int2* metriche, const int max_adj_dept)
	{
		int current_node_index = get_global_id(0);
		if(current_node_index >= n_nodes) return;
		
		[...] //omissis of various security checks
		
		for (int j = 0; j < max_adj_dept; j++) {
			int parentAdjIndex = j;
			matrixToArrayIndex = matrix_to_array_indexes(parentAdjIndex, current_node_index, n_nodes);
			int edge_weight = 1;
			int parent_index = edges[matrixToArrayIndex];
			if (parent_index >= 0){
				int weight_with_this_parent = edge_weight + metriche[parent_index].x + nodes[current_node_index];
				int level_with_this_parent = metriche[parent_index].y + 1;
				metrics_with_this_parent = (int2)(weight_with_this_parent, level_with_this_parent);
				if (gt(metrics_with_this_parent, metriche[current_node_index]))
				metriche[current_node_index] = metrics_with_this_parent;
			}
			int child_index = edges_reverse[matrixToArrayIndex];
			if (child_index >= 0)
			atomic_inc(&next_queue_[child_index]);
		}
	}
\end{lstlisting}

\subsection{Ordinamento dei task}
\begin{lstlisting}[language=C++, caption={MergeSort kernel for metrics couple array, source: \url{https://github.com/Gram21/GPUSorting}},captionpos=b]
	__kernel void merge_sort(const __global int2* inArray, __global int2* outArray, const uint stride, const uint size)
	{
		const uint baseIndex = get_global_id(0) * stride;
		if ((baseIndex + stride) > size) return;
		const char dir = 1;
		uint middle = baseIndex + (stride >> 1);
		uint left = baseIndex;
		uint right = middle;
		bool selectLeft;
		
		for (uint i = baseIndex; i < (baseIndex + stride); i++) {
			selectLeft = (left < middle && (right == (baseIndex + stride) || lte(inArray[left], inArray[right]))) == dir;
			
			outArray[i] = (selectLeft) ? inArray[left] : inArray[right];
			
			left += selectLeft;
			right += 1 - selectLeft;
		}
	}
\end{lstlisting}

\subsection{Selezione del processore}
\begin{lstlisting}[language=C++, caption={Compute metrics kernel II},captionpos=b]
	void ScheduleTasksOnProcessors()
	{
		for (int i = 0; i < metrics_len; i++)
		{
			int current_node = ordered_metrics[i].z; 
			if (current_node >= n_nodes) continue;
			int predecessor_with_max_aft = -1;
			int max_aft_of_predecessors = -1;
			int processor_for_max_aft_predecessor = -1;
			int weight_for_max_aft_predecessor = 0;

			for (int j = 0; j < DAG->max_parents_for_nodes; j++)
			{
				int currentParent = edges[matrix_to_array_indexes(j, current_node, DAG->len)];
				if (currentParent > -1) {
					int edge_weight_with_parent = predecessors[matrix_to_array_indexes(j, current_node, DAG->len)];
					int parentEFT = task_processor_assignment[currentParent].z + edge_weight_with_parent;
					if (parentEFT > max_aft_of_predecessors) {
						max_aft_of_predecessors = parentEFT;
						predecessor_with_max_aft = currentParent;
						processor_for_max_aft_predecessor = task_processor_assignment[currentParent].x;
						weight_for_max_aft_predecessor = edge_weight_with_parent;
					}
				}
			}
			
			int eft_min = INT_MAX;
			
			int cost_of_predecessors_in_different_processors = 0;
			int remaining_transfer_cost = 0;
			for (int j = 0; j < DAG->max_parents_for_nodes; j++)
			{
				int currentParent = edges[matrix_to_array_indexes(j, current_node, DAG->len)];
				if (currentParent > -1 && currentParent != predecessor_with_max_aft) {
					cost_of_predecessors_in_different_processors = max(
					cost_of_predecessors_in_different_processors,
					task_processor_assignment[currentParent].z + predecessors[matrix_to_array_indexes(j, current_node, DAG->len)]);
				}
			}
			
			for (int processor = 0; processor < DAG->number_of_processors; processor++) {
				int cost_of_predecessor_in_same_processor = 0;
				int cost_on_processor = costs[matrix_to_array_indexes(current_node, processor, DAG->number_of_processors)];
				if (processor_for_max_aft_predecessor == processor) {
					cost_of_predecessor_in_same_processor = weight_for_max_aft_predecessor;
				}
				remaining_transfer_cost = max(max_aft_of_predecessors - cost_of_predecessor_in_same_processor, cost_of_predecessors_in_different_processors);
				
				int est = max(processorsNextSlotStart[processor], remaining_transfer_cost);
				int eft = est + cost_on_processor;
				if (eft < eft_min) {
					eft_min = eft;
					task_processor_assignment[current_node] = cl_int3{ processor, est, eft };
				}
			}
			
			processorsNextSlotStart[task_processor_assignment[current_node].x] = task_processor_assignment[current_node].z;
		}
	}
\end{lstlisting}
\section{Ottimizzazioni}